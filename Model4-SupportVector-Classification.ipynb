{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model 4: Support Vector Classifier (SVC)\n",
    "## Advanced Order Completion Prediction\n",
    "\n",
    "### Objective:\n",
    "Build a **Support Vector Classifier** to predict order completion status with improved performance over Logistic Regression.\n",
    "\n",
    "### Why Support Vector Machine (SVM)?\n",
    "- **Powerful Non-Linear Classifier**: Handles complex decision boundaries\n",
    "- **Kernel Trick**: Transforms data to higher dimensions\n",
    "- **Memory Efficient**: Uses subset of training points (support vectors)\n",
    "- **Robust to Outliers**: Less affected by extreme values\n",
    "- **Strong Theory**: Solid mathematical foundation\n",
    "- **Industry Standard**: Used in fraud detection, customer analytics\n",
    "\n",
    "### SVM vs Logistic Regression:\n",
    "| Aspect | Logistic Regression | Support Vector Machine |\n",
    "|--------|---------------------|------------------------|\n",
    "| **Decision Boundary** | Linear only | Linear + Non-linear (kernels) |\n",
    "| **Complexity** | Simple | Medium-High |\n",
    "| **Training Speed** | Fast | Slower |\n",
    "| **Accuracy** | Good (baseline) | Better (complex patterns) |\n",
    "| **Interpretability** | High | Medium |\n",
    "| **Outlier Sensitivity** | Moderate | Low |\n",
    "\n",
    "### SVM Kernels:\n",
    "- **Linear**: For linearly separable data\n",
    "- **RBF (Radial Basis Function)**: Most popular, handles non-linearity\n",
    "- **Polynomial**: For polynomial relationships\n",
    "- **Sigmoid**: Neural network-like behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(\"ðŸš€ Ready for Support Vector Classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "## Step 2: Load the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/cleaned_final_data.csv')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Total Rows: {len(df):,}\")\n",
    "print(f\"Total Columns: {len(df.columns)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check status distribution\n",
    "print(\"\\nOrder Status Distribution:\")\n",
    "print(df['status'].value_counts())\n",
    "print(\"\\nPercentage:\")\n",
    "print(df['status'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "## Step 3: Create Binary Target & Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target: 1 if complete, 0 otherwise\n",
    "df['is_complete'] = (df['status'] == 'complete').astype(int)\n",
    "\n",
    "print(\"Binary Target Created:\")\n",
    "print(df['is_complete'].value_counts())\n",
    "print(\"\\nClass Distribution:\")\n",
    "class_dist = df['is_complete'].value_counts(normalize=True) * 100\n",
    "print(f\"  Not Complete (0): {class_dist[0]:.2f}%\")\n",
    "print(f\"  Complete (1): {class_dist[1]:.2f}%\")\n",
    "\n",
    "# Check for class imbalance\n",
    "if abs(class_dist[0] - class_dist[1]) > 30:\n",
    "    print(\"\\nâš  WARNING: Significant class imbalance detected!\")\n",
    "    print(\"   Consider using class_weight='balanced' in SVC\")\n",
    "else:\n",
    "    print(\"\\nâœ“ Classes are reasonably balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "### Select Features for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "feature_columns = [\n",
    "    'price',\n",
    "    'qty_ordered',\n",
    "    'discount_amount',\n",
    "    'month',\n",
    "    'category_name_1',\n",
    "    'payment_method'\n",
    "]\n",
    "\n",
    "target_column = 'is_complete'\n",
    "\n",
    "# Create modeling dataset\n",
    "df_model = df[feature_columns + [target_column]].copy()\n",
    "df_model = df_model.dropna()\n",
    "\n",
    "print(f\"âœ“ Model dataset shape: {df_model.shape}\")\n",
    "print(f\"âœ“ Features: {len(feature_columns)}\")\n",
    "print(f\"âœ“ Target: {target_column}\")\n",
    "print(f\"âœ“ Missing values: {df_model.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5",
   "metadata": {},
   "source": [
    "### Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize label encoders\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['category_name_1', 'payment_method']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_model[col] = le.fit_transform(df_model[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"âœ“ Encoded {col}: {df_model[col].nunique()} unique values\")\n",
    "\n",
    "print(\"\\nâœ“ All categorical variables encoded!\")\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6",
   "metadata": {},
   "source": [
    "## Step 4: Data Splitting & Feature Scaling\n",
    "\n",
    "**âš ï¸ CRITICAL**: SVMs are **very sensitive** to feature scales. We must standardize features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_model[feature_columns]\n",
    "y = df_model[target_column]\n",
    "\n",
    "# Split data: 80% train, 20% test (stratified to maintain class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA SPLIT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(\"\\nClass distribution in training:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "print(\"\\nClass distribution in testing:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)\n",
    "\n",
    "# Feature Scaling (CRITICAL FOR SVM!)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE SCALING (Essential for SVM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ“ Features standardized (mean=0, std=1)\")\n",
    "print(\"\\nScaling Statistics:\")\n",
    "print(f\"  Mean before scaling: {X_train.mean().mean():.2f}\")\n",
    "print(f\"  Mean after scaling: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"  Std before scaling: {X_train.std().mean():.2f}\")\n",
    "print(f\"  Std after scaling: {X_train_scaled.std():.6f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7",
   "metadata": {},
   "source": [
    "## Step 5: Train Support Vector Classifier with RBF Kernel\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- **C**: Regularization parameter (higher = less regularization)\n",
    "- **kernel**: Type of kernel function ('rbf', 'linear', 'poly', 'sigmoid')\n",
    "- **gamma**: Kernel coefficient (higher = more influence per training point)\n",
    "- **class_weight**: Handle class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Support Vector Classifier with RBF kernel\n",
    "svc_model = SVC(\n",
    "    kernel='rbf',              # Radial Basis Function (best for non-linear)\n",
    "    C=1.0,                     # Regularization parameter\n",
    "    gamma='scale',             # Kernel coefficient (auto-scaled)\n",
    "    probability=True,          # Enable probability estimates\n",
    "    random_state=42,           # Reproducibility\n",
    "    class_weight='balanced'    # Handle class imbalance\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUPPORT VECTOR CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"  Kernel: {svc_model.kernel.upper()}\")\n",
    "print(f\"  C (Regularization): {svc_model.C}\")\n",
    "print(f\"  Gamma: {svc_model.gamma}\")\n",
    "print(f\"  Class Weight: {svc_model.class_weight}\")\n",
    "print(f\"  Probability: {svc_model.probability}\")\n",
    "print(\"\\nTraining in progress...\")\n",
    "print(\"(This may take 5-10 minutes for large dataset)\\n\")\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "svc_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ“ MODEL TRAINED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"Support vectors: {svc_model.n_support_}\")\n",
    "print(f\"Total support vectors: {svc_model.support_vectors_.shape[0]:,}\")\n",
    "print(f\"Percentage of training data: {(svc_model.support_vectors_.shape[0]/len(X_train))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8",
   "metadata": {},
   "source": [
    "## Step 6: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "y_train_pred = svc_model.predict(X_train_scaled)\n",
    "y_test_pred = svc_model.predict(X_test_scaled)\n",
    "\n",
    "# Get probability estimates\n",
    "y_train_proba = svc_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba = svc_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"âœ“ Predictions completed!\")\n",
    "print(f\"\\nPrediction Examples (First 10):\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test[:10].values,\n",
    "    'Predicted': y_test_pred[:10],\n",
    "    'Probability': y_test_proba[:10].round(4),\n",
    "    'Correct': (y_test[:10].values == y_test_pred[:10])\n",
    "})\n",
    "comparison_df['Actual'] = comparison_df['Actual'].map({0: 'Not Complete', 1: 'Complete'})\n",
    "comparison_df['Predicted'] = comparison_df['Predicted'].map({0: 'Not Complete', 1: 'Complete'})\n",
    "comparison_df['Correct'] = comparison_df['Correct'].map({True: 'âœ“', False: 'âœ—'})\n",
    "\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation\n",
    "\n",
    "### Classification Metrics:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: True positives / (True positives + False positives)\n",
    "- **Recall**: True positives / (True positives + False negatives)\n",
    "- **F1-Score**: Harmonic mean of Precision and Recall\n",
    "- **AUC-ROC**: Area under ROC curve (0.5-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "train_auc = roc_auc_score(y_train, y_train_proba)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“Š TRAINING SET:\")\n",
    "print(f\"  Accuracy:  {train_accuracy:.4f}\")\n",
    "print(f\"  Precision: {train_precision:.4f}\")\n",
    "print(f\"  Recall:    {train_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {train_f1:.4f}\")\n",
    "print(f\"  AUC-ROC:   {train_auc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š TESTING SET (Main Metrics):\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f} â­\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "print(f\"  AUC-ROC:   {test_auc:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Performance assessment\n",
    "print(\"\\nðŸ’¡ MODEL QUALITY ASSESSMENT:\")\n",
    "if test_accuracy >= 0.90:\n",
    "    quality = \"EXCELLENT\"\n",
    "elif test_accuracy >= 0.85:\n",
    "    quality = \"VERY GOOD\"\n",
    "elif test_accuracy >= 0.80:\n",
    "    quality = \"GOOD\"\n",
    "elif test_accuracy >= 0.75:\n",
    "    quality = \"FAIR\"\n",
    "else:\n",
    "    quality = \"NEEDS IMPROVEMENT\"\n",
    "\n",
    "print(f\"  Overall Quality: {quality}\")\n",
    "print(f\"  Correctly Classified: {test_accuracy*100:.2f}%\")\n",
    "print(f\"  AUC Score: {test_auc:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                          target_names=['Not Complete', 'Complete'],\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step10",
   "metadata": {},
   "source": [
    "## Step 8: Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Visualize confusion matrix with percentages\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute numbers\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax1,\n",
    "            xticklabels=['Not Complete', 'Complete'],\n",
    "            yticklabels=['Not Complete', 'Complete'])\n",
    "ax1.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Confusion Matrix (Counts)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Percentages\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_percent, annot=True, fmt='.2f', cmap='Greens', cbar=True, ax=ax2,\n",
    "            xticklabels=['Not Complete', 'Complete'],\n",
    "            yticklabels=['Not Complete', 'Complete'])\n",
    "ax2.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Confusion Matrix (Percentages)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed breakdown\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"=\"*60)\n",
    "print(\"CONFUSION MATRIX BREAKDOWN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True Negatives (TN):   {tn:,} - Correctly predicted Not Complete\")\n",
    "print(f\"False Positives (FP):  {fp:,} - Incorrectly predicted Complete\")\n",
    "print(f\"False Negatives (FN):  {fn:,} - Incorrectly predicted Not Complete\")\n",
    "print(f\"True Positives (TP):   {tp:,} - Correctly predicted Complete\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Additional metrics\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"\\nSpecificity (True Negative Rate): {specificity:.4f}\")\n",
    "print(f\"Sensitivity (Recall/True Positive Rate): {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step11",
   "metadata": {},
   "source": [
    "## Step 9: ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc_curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "\n",
    "# Find optimal threshold (Youden's index)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'SVC (AUC = {test_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier (AUC = 0.5000)')\n",
    "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='green', s=200,\n",
    "           label=f'Optimal Threshold = {optimal_threshold:.3f}', zorder=5)\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curve - Support Vector Classifier', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ROC CURVE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AUC-ROC Score: {test_auc:.4f}\")\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if test_auc >= 0.90:\n",
    "    print(\"  âœ“ EXCELLENT discrimination ability\")\n",
    "elif test_auc >= 0.80:\n",
    "    print(\"  âœ“ GOOD discrimination ability\")\n",
    "elif test_auc >= 0.70:\n",
    "    print(\"  â—‹ FAIR discrimination ability\")\n",
    "else:\n",
    "    print(\"  âœ— POOR discrimination ability\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step12",
   "metadata": {},
   "source": [
    "## Step 10: Model Comparison with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Training Logistic Regression for comparison...\")\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "lr_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate LR metrics\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "lr_precision = precision_score(y_test, lr_pred)\n",
    "lr_recall = recall_score(y_test, lr_pred)\n",
    "lr_f1 = f1_score(y_test, lr_pred)\n",
    "lr_auc = roc_auc_score(y_test, lr_proba)\n",
    "\n",
    "# Comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'],\n",
    "    'Logistic Regression': [lr_accuracy, lr_precision, lr_recall, lr_f1, lr_auc],\n",
    "    'Support Vector Machine': [test_accuracy, test_precision, test_recall, test_f1, test_auc],\n",
    "    'Difference': [\n",
    "        f\"{((test_accuracy - lr_accuracy) / lr_accuracy * 100):+.2f}%\",\n",
    "        f\"{((test_precision - lr_precision) / lr_precision * 100):+.2f}%\",\n",
    "        f\"{((test_recall - lr_recall) / lr_recall * 100):+.2f}%\",\n",
    "        f\"{((test_f1 - lr_f1) / lr_f1 * 100):+.2f}%\",\n",
    "        f\"{((test_auc - lr_auc) / lr_auc * 100):+.2f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON: SVM vs LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visual comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "lr_scores = [lr_accuracy, lr_precision, lr_recall, lr_f1, lr_auc]\n",
    "svm_scores = [test_accuracy, test_precision, test_recall, test_f1, test_auc]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, lr_scores, width, label='Logistic Regression', \n",
    "               alpha=0.8, edgecolor='black', color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, svm_scores, width, label='Support Vector Machine', \n",
    "               alpha=0.8, edgecolor='black', color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Performance Comparison: SVM vs Logistic Regression', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Winner announcement\n",
    "print(\"\\nðŸ† MODEL COMPARISON SUMMARY:\")\n",
    "svm_wins = sum([test_accuracy > lr_accuracy, test_precision > lr_precision, \n",
    "               test_recall > lr_recall, test_f1 > lr_f1, test_auc > lr_auc])\n",
    "print(f\"   SVM wins in {svm_wins}/5 metrics\")\n",
    "if svm_wins >= 3:\n",
    "    print(\"   âœ“ Support Vector Machine is the WINNER!\")\n",
    "elif svm_wins == 2:\n",
    "    print(\"   â‰ˆ Models perform similarly\")\n",
    "else:\n",
    "    print(\"   âœ“ Logistic Regression is the WINNER!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step13",
   "metadata": {},
   "source": [
    "## Step 11: Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "additional_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Probability Distribution\n",
    "axes[0, 0].hist([y_test_proba[y_test == 0], y_test_proba[y_test == 1]], \n",
    "               bins=30, label=['Not Complete', 'Complete'], \n",
    "               alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Predicted Probability', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Distribution of Predicted Probabilities', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "axes[0, 1].plot(recall_vals, precision_vals, linewidth=2, color='purple')\n",
    "axes[0, 1].set_xlabel('Recall', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Precision', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Precision-Recall Curve', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].fill_between(recall_vals, precision_vals, alpha=0.2, color='purple')\n",
    "\n",
    "# 3. Calibration Curve (Reliability Diagram)\n",
    "from sklearn.calibration import calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_test, y_test_proba, n_bins=10\n",
    ")\n",
    "axes[1, 0].plot(mean_predicted_value, fraction_of_positives, 'o-', \n",
    "               linewidth=2, label='SVC', color='blue')\n",
    "axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "axes[1, 0].set_xlabel('Mean Predicted Probability', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Fraction of Positives', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Calibration Curve', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Class-wise Performance\n",
    "class_perf = pd.DataFrame({\n",
    "    'Class': ['Not Complete', 'Complete'],\n",
    "    'Precision': precision_score(y_test, y_test_pred, average=None),\n",
    "    'Recall': recall_score(y_test, y_test_pred, average=None),\n",
    "    'F1-Score': f1_score(y_test, y_test_pred, average=None)\n",
    "})\n",
    "\n",
    "class_perf.set_index('Class').plot(kind='bar', ax=axes[1, 1], \n",
    "                                   alpha=0.8, edgecolor='black', rot=0)\n",
    "axes[1, 1].set_xlabel('Class', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Class-wise Performance Metrics', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].legend(loc='lower right', fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ All visualizations generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step14",
   "metadata": {},
   "source": [
    "## Step 12: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save SVC model\n",
    "with open('support_vector_classifier.pkl', 'wb') as file:\n",
    "    pickle.dump(svc_model, file)\n",
    "\n",
    "# Save scaler\n",
    "with open('scaler_svc.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# Save label encoders\n",
    "with open('label_encoders_svc.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoders, file)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ“ MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Saved files:\")\n",
    "print(\"  - support_vector_classifier.pkl\")\n",
    "print(\"  - scaler_svc.pkl\")\n",
    "print(\"  - label_encoders_svc.pkl\")\n",
    "\n",
    "print(\"\\nðŸ’¡ MODEL SUMMARY:\")\n",
    "print(f\"  Algorithm: Support Vector Classifier (RBF Kernel)\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  AUC-ROC: {test_auc:.4f}\")\n",
    "print(f\"  Training samples: {len(X_train):,}\")\n",
    "print(f\"  Support vectors: {svc_model.support_vectors_.shape[0]:,}\")\n",
    "print(f\"  Features: {len(feature_columns)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Conclusion\n",
    "\n",
    "### What We Accomplished:\n",
    "1. âœ… Trained Support Vector Classifier with RBF kernel\n",
    "2. âœ… Compared performance with Logistic Regression\n",
    "3. âœ… Comprehensive evaluation with multiple metrics\n",
    "4. âœ… ROC curve and confusion matrix analysis\n",
    "5. âœ… Calibration curve and probability analysis\n",
    "6. âœ… Model saved for deployment\n",
    "\n",
    "### Key Insights:\n",
    "- **Non-Linear Classification**: SVM handles complex decision boundaries\n",
    "- **Kernel Trick**: Transforms data to higher dimensions\n",
    "- **Memory Efficient**: Uses only support vectors\n",
    "- **Robust**: Less sensitive to outliers\n",
    "\n",
    "### SVM Advantages:\n",
    "âœ… **Complex Patterns**: Captures non-linear relationships  \n",
    "âœ… **High Dimensional**: Works well with many features  \n",
    "âœ… **Theoretical Foundation**: Strong mathematical basis  \n",
    "âœ… **Robust**: Less prone to overfitting with proper regularization  \n",
    "\n",
    "### When to Use SVM:\n",
    "- Binary classification problems\n",
    "- Non-linear decision boundaries\n",
    "- High-dimensional data\n",
    "- Small to medium datasets\n",
    "- When accuracy is critical\n",
    "\n",
    "### SVM Limitations:\n",
    "âš  **Slow Training**: O(nÂ²) to O(nÂ³) complexity  \n",
    "âš  **Memory Intensive**: For very large datasets  \n",
    "âš  **Hyperparameter Sensitive**: Requires tuning  \n",
    "âš  **Black Box**: Less interpretable than logistic regression  \n",
    "\n",
    "### Model Comparison Summary:\n",
    "\n",
    "| Model | Pros | Cons | Best For |\n",
    "|-------|------|------|----------|\n",
    "| **Logistic Regression** | Fast, interpretable, simple | Linear only | Baseline, explainability |\n",
    "| **Support Vector Machine** | Non-linear, robust, accurate | Slow, needs tuning | Complex patterns, accuracy |\n",
    "\n",
    "### Next Steps:\n",
    "- Hyperparameter tuning (GridSearchCV for C, gamma)\n",
    "- Try different kernels (poly, sigmoid)\n",
    "- Feature selection for faster training\n",
    "- Ensemble with other models\n",
    "- Deploy to production\n",
    "\n",
    "---\n",
    "**Congratulations on mastering Support Vector Machines! ðŸš€ðŸŽ¯**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
